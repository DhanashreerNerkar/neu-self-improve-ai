{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074c1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple LLM wrapper - start with rules, then add real model\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "class SimpleLLM:\n",
    "    \"\"\"Mock LLM for initial testing\"\"\"\n",
    "    \n",
    "    def __init__(self, use_real_model=False):\n",
    "        self.use_real_model = use_real_model\n",
    "    \n",
    "    def get_object_prior(self, object_name: str, locations: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Get probability distribution for where object might be\"\"\"\n",
    "        # Hand-coded rules for common objects\n",
    "        priors = {\n",
    "            \"apple\": {\"kitchen\": 0.5, \"fridge\": 0.3, \"pantry\": 0.2},\n",
    "            \"book\": {\"bedroom\": 0.4, \"living_room\": 0.4, \"office\": 0.2},\n",
    "            \"keys\": {\"entrance\": 0.4, \"bedroom\": 0.3, \"kitchen\": 0.3},\n",
    "            \"plate\": {\"kitchen\": 0.6, \"dishwasher\": 0.3, \"cabinet\": 0.1}\n",
    "        }\n",
    "        \n",
    "        if object_name in priors:\n",
    "            result = {}\n",
    "            for loc in locations:\n",
    "                for key in priors[object_name]:\n",
    "                    if key in loc.lower() or loc.lower() in key:\n",
    "                        result[loc] = priors[object_name].get(key, 0)\n",
    "            \n",
    "            if result:\n",
    "                total = sum(result.values())\n",
    "                return {k: v/total for k, v in result.items()}\n",
    "        \n",
    "        # Uniform distribution for unknown objects\n",
    "        uniform_prob = 1.0 / len(locations)\n",
    "        return {loc: uniform_prob for loc in locations}\n",
    "    \n",
    "    def suggest_action(self, current_state: str, goal: str, history: List[str]) -> str:\n",
    "        \"\"\"Suggest next action based on state and goal\"\"\"\n",
    "        # Simple heuristic for now\n",
    "        actions = [\"move_to_kitchen\", \"move_to_bedroom\", \"search_area\", \"pick_object\"]\n",
    "        return random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902a8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplified MCTS implementation for CPU\n",
    "\"\"\"\n",
    "import random\n",
    "import math\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    state: Dict\n",
    "    action: Optional[str]\n",
    "    parent: Optional['Node']\n",
    "    children: List['Node'] = field(default_factory=list)\n",
    "    visits: int = 0\n",
    "    value: float = 0.0\n",
    "    \n",
    "class SimpleMCTS:\n",
    "    def __init__(self, llm, c_param=1.4, max_simulations=50):\n",
    "        self.llm = llm\n",
    "        self.c_param = c_param\n",
    "        self.max_simulations = max_simulations\n",
    "    \n",
    "    def search(self, initial_state: Dict, goal: Dict, available_actions: List[str]) -> str:\n",
    "        \"\"\"Run MCTS and return best action\"\"\"\n",
    "        root = Node(state=initial_state, action=None, parent=None)\n",
    "        \n",
    "        for sim in range(self.max_simulations):\n",
    "            # Selection\n",
    "            node = self._select(root)\n",
    "            \n",
    "            # Expansion\n",
    "            if node.visits > 0 and not self._is_terminal(node.state, goal):\n",
    "                node = self._expand(node, available_actions)\n",
    "            \n",
    "            # Simulation\n",
    "            reward = self._simulate(node.state, goal)\n",
    "            \n",
    "            # Backpropagation\n",
    "            self._backpropagate(node, reward)\n",
    "        \n",
    "        # Return best action\n",
    "        return self._best_action(root)\n",
    "    \n",
    "    def _select(self, node: Node) -> Node:\n",
    "        \"\"\"Select node using UCT\"\"\"\n",
    "        while node.children:\n",
    "            if all(child.visits > 0 for child in node.children):\n",
    "                node = self._uct_select(node)\n",
    "            else:\n",
    "                # Return first unvisited child\n",
    "                return next(c for c in node.children if c.visits == 0)\n",
    "        return node\n",
    "    \n",
    "    def _uct_select(self, node: Node) -> Node:\n",
    "        \"\"\"Select child with highest UCT value\"\"\"\n",
    "        best_value = -float('inf')\n",
    "        best_node = None\n",
    "        \n",
    "        for child in node.children:\n",
    "            if child.visits == 0:\n",
    "                return child\n",
    "            uct_value = (child.value / child.visits + \n",
    "                        self.c_param * math.sqrt(math.log(node.visits) / child.visits))\n",
    "            if uct_value > best_value:\n",
    "                best_value = uct_value\n",
    "                best_node = child\n",
    "        \n",
    "        return best_node\n",
    "    \n",
    "    def _expand(self, node: Node, actions: List[str]) -> Node:\n",
    "        \"\"\"Expand node with one child\"\"\"\n",
    "        # Get LLM suggestion for which action to try\n",
    "        suggested_action = self.llm.suggest_action(\n",
    "            str(node.state), \n",
    "            \"goal\", \n",
    "            []\n",
    "        )\n",
    "        \n",
    "        # If LLM suggestion is not in available actions, pick random\n",
    "        if suggested_action not in actions:\n",
    "            suggested_action = random.choice(actions)\n",
    "        \n",
    "        # Create child node\n",
    "        new_state = self._apply_action(node.state.copy(), suggested_action)\n",
    "        child = Node(\n",
    "            state=new_state,\n",
    "            action=suggested_action,\n",
    "            parent=node\n",
    "        )\n",
    "        node.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def _simulate(self, state: Dict, goal: Dict) -> float:\n",
    "        \"\"\"Random rollout from state\"\"\"\n",
    "        current_state = state.copy()\n",
    "        steps = 0\n",
    "        max_steps = 20\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            if self._is_terminal(current_state, goal):\n",
    "                return 1.0 - (steps / max_steps)  # Reward based on speed\n",
    "            \n",
    "            # Random action\n",
    "            action = random.choice([\"move\", \"pick\", \"search\"])\n",
    "            current_state = self._apply_action(current_state, action)\n",
    "            steps += 1\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def _backpropagate(self, node: Node, reward: float):\n",
    "        \"\"\"Update values up the tree\"\"\"\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value += reward\n",
    "            node = node.parent\n",
    "    \n",
    "    def _best_action(self, root: Node) -> str:\n",
    "        \"\"\"Select most visited child\"\"\"\n",
    "        if not root.children:\n",
    "            return \"no_action\"\n",
    "        \n",
    "        best_child = max(root.children, key=lambda c: c.visits)\n",
    "        return best_child.action if best_child.action else \"no_action\"\n",
    "    \n",
    "    def _is_terminal(self, state: Dict, goal: Dict) -> bool:\n",
    "        \"\"\"Check if goal is reached\"\"\"\n",
    "        # Simple check - customize based on your task\n",
    "        for key, value in goal.items():\n",
    "            if state.get(key) != value:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _apply_action(self, state: Dict, action: str) -> Dict:\n",
    "        \"\"\"Apply action to state (simplified)\"\"\"\n",
    "        new_state = state.copy()\n",
    "        # This is task-specific - implement based on your domain\n",
    "        if \"move\" in action:\n",
    "            new_state[\"last_action\"] = action\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33bf47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple object finding task for testing\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class ObjectFindingTask:\n",
    "    \"\"\"Simple task: find objects in house\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rooms = [\"kitchen\", \"bedroom\", \"living_room\", \"bathroom\", \"office\"]\n",
    "        self.objects = [\"keys\", \"phone\", \"wallet\", \"book\", \"apple\"]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to random state\"\"\"\n",
    "        self.robot_location = \"entrance\"\n",
    "        self.object_locations = {\n",
    "            obj: random.choice(self.rooms) \n",
    "            for obj in self.objects\n",
    "        }\n",
    "        self.holding = None\n",
    "        self.steps_taken = 0\n",
    "        self.found_objects = set()\n",
    "    \n",
    "    def get_state(self) -> Dict:\n",
    "        \"\"\"Get current partially observable state\"\"\"\n",
    "        state = {\n",
    "            \"robot_location\": self.robot_location,\n",
    "            \"holding\": self.holding,\n",
    "            \"visible_objects\": self._get_visible_objects(),\n",
    "            \"found_objects\": list(self.found_objects)\n",
    "        }\n",
    "        return state\n",
    "    \n",
    "    def _get_visible_objects(self) -> List[str]:\n",
    "        \"\"\"Objects visible in current room\"\"\"\n",
    "        visible = []\n",
    "        for obj, loc in self.object_locations.items():\n",
    "            if loc == self.robot_location:\n",
    "                visible.append(obj)\n",
    "        return visible\n",
    "    \n",
    "    def execute_action(self, action: str) -> Tuple[Dict, float, bool]:\n",
    "        \"\"\"Execute action and return new state, reward, done\"\"\"\n",
    "        self.steps_taken += 1\n",
    "        reward = -0.01  # Small penalty for each step\n",
    "        \n",
    "        if action.startswith(\"move_to_\"):\n",
    "            room = action.replace(\"move_to_\", \"\")\n",
    "            if room in self.rooms:\n",
    "                self.robot_location = room\n",
    "                reward = -0.01  # Movement cost\n",
    "        \n",
    "        elif action.startswith(\"pick_\"):\n",
    "            obj = action.replace(\"pick_\", \"\")\n",
    "            if obj in self._get_visible_objects() and not self.holding:\n",
    "                self.holding = obj\n",
    "                self.found_objects.add(obj)\n",
    "                reward = 0.5  # Reward for picking up object\n",
    "        \n",
    "        elif action == \"search\":\n",
    "            # Just costs time\n",
    "            reward = -0.02\n",
    "        \n",
    "        # Check if task is done\n",
    "        done = self.steps_taken >= 50 or len(self.found_objects) >= 3\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "    \n",
    "    def get_available_actions(self) -> List[str]:\n",
    "        \"\"\"Get list of valid actions in current state\"\"\"\n",
    "        actions = [f\"move_to_{room}\" for room in self.rooms]\n",
    "        actions.append(\"search\")\n",
    "        \n",
    "        if self._get_visible_objects() and not self.holding:\n",
    "            for obj in self._get_visible_objects():\n",
    "                actions.append(f\"pick_{obj}\")\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495f200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLM-MCTS Experiment\n",
      "--------------------------------------------------\n",
      "\n",
      "Running Random Baseline...\n",
      "------------------------------\n",
      "Trial 1: Reward=-0.060, Steps=50\n",
      "Trial 2: Reward=-0.120, Steps=50\n",
      "Trial 3: Reward=-0.080, Steps=50\n",
      "Trial 4: Reward=-0.060, Steps=50\n",
      "Trial 5: Reward=-0.080, Steps=50\n",
      "Random Average: Reward=-0.080, Steps=50.0\n",
      "\n",
      "Running LLM-MCTS...\n",
      "------------------------------\n",
      "Trial 1: Reward=-0.030, Steps=50\n",
      "Trial 2: Reward=-0.050, Steps=50\n",
      "Trial 3: Reward=-0.050, Steps=50\n",
      "Trial 4: Reward=-0.040, Steps=50\n",
      "Trial 5: Reward=-0.020, Steps=50\n",
      "LLM-MCTS Average: Reward=-0.038, Steps=50.0\n",
      "\n",
      "==================================================\n",
      "RESULTS SUMMARY\n",
      "==================================================\n",
      "Random Agent:   Reward=-0.080, Steps=50.0\n",
      "LLM-MCTS Agent: Reward=-0.038, Steps=50.0\n",
      "Improvement:    52.5%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main experiment runner\n",
    "\"\"\"\n",
    "import random\n",
    "import time\n",
    "from simple_llm import SimpleLLM\n",
    "from simple_mcts import SimpleMCTS\n",
    "from task_environment import ObjectFindingTask\n",
    "\n",
    "def run_baseline_random(task, max_steps=50):\n",
    "    \"\"\"Random agent baseline\"\"\"\n",
    "    task.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        actions = task.get_available_actions()\n",
    "        action = random.choice(actions)\n",
    "        state, reward, done = task.execute_action(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward, steps\n",
    "\n",
    "def run_llm_mcts(task, llm, mcts, max_steps=50):\n",
    "    \"\"\"Run LLM-MCTS agent\"\"\"\n",
    "    task.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        state = task.get_state()\n",
    "        goal = {\"find_objects\": True}  # Simplified goal\n",
    "        \n",
    "        # Get available actions\n",
    "        actions = task.get_available_actions()\n",
    "        \n",
    "        # Run MCTS to get best action\n",
    "        action = mcts.search(state, goal, actions)\n",
    "        \n",
    "        # Execute action\n",
    "        state, reward, done = task.execute_action(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward, steps\n",
    "\n",
    "def main():\n",
    "    print(\"Starting LLM-MCTS Experiment\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Initialize components\n",
    "    llm = SimpleLLM(use_real_model=False)  # Start with mock\n",
    "    mcts = SimpleMCTS(llm, max_simulations=20)  # Small for CPU\n",
    "    task = ObjectFindingTask()\n",
    "    \n",
    "    # Run experiments\n",
    "    n_trials = 5\n",
    "    \n",
    "    print(\"\\nRunning Random Baseline...\")\n",
    "    print(\"-\" * 30)\n",
    "    random_rewards = []\n",
    "    random_steps = []\n",
    "    for i in range(n_trials):\n",
    "        reward, steps = run_baseline_random(task)\n",
    "        random_rewards.append(reward)\n",
    "        random_steps.append(steps)\n",
    "        print(f\"Trial {i+1}: Reward={reward:.3f}, Steps={steps}\")\n",
    "    \n",
    "    avg_random_reward = sum(random_rewards) / len(random_rewards)\n",
    "    avg_random_steps = sum(random_steps) / len(random_steps)\n",
    "    print(f\"Random Average: Reward={avg_random_reward:.3f}, Steps={avg_random_steps:.1f}\")\n",
    "    \n",
    "    print(\"\\nRunning LLM-MCTS...\")\n",
    "    print(\"-\" * 30)\n",
    "    mcts_rewards = []\n",
    "    mcts_steps = []\n",
    "    for i in range(n_trials):\n",
    "        reward, steps = run_llm_mcts(task, llm, mcts)\n",
    "        mcts_rewards.append(reward)\n",
    "        mcts_steps.append(steps)\n",
    "        print(f\"Trial {i+1}: Reward={reward:.3f}, Steps={steps}\")\n",
    "    \n",
    "    avg_mcts_reward = sum(mcts_rewards) / len(mcts_rewards)\n",
    "    avg_mcts_steps = sum(mcts_steps) / len(mcts_steps)\n",
    "    print(f\"LLM-MCTS Average: Reward={avg_mcts_reward:.3f}, Steps={avg_mcts_steps:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Random Agent:   Reward={avg_random_reward:.3f}, Steps={avg_random_steps:.1f}\")\n",
    "    print(f\"LLM-MCTS Agent: Reward={avg_mcts_reward:.3f}, Steps={avg_mcts_steps:.1f}\")\n",
    "    print(f\"Improvement:    {((avg_mcts_reward - avg_random_reward) / abs(avg_random_reward) * 100):.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)  # For reproducibility\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
