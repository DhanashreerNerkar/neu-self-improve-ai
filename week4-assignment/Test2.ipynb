{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "043d5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8994c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLLM:\n",
    "    \"\"\"LLM wrapper with both mock and real model options\"\"\"\n",
    "    \n",
    "    def __init__(self, use_real_model=False, model_name=\"gpt2\"):\n",
    "        self.use_real_model = use_real_model\n",
    "        \n",
    "        if use_real_model:\n",
    "            try:\n",
    "                from transformers import pipeline\n",
    "                print(f\"Loading {model_name}... (this may take a moment)\")\n",
    "                self.generator = pipeline(\n",
    "                    'text-generation', \n",
    "                    model=model_name,\n",
    "                    device=-1,  # Force CPU\n",
    "                    pad_token_id=50256  # For GPT2\n",
    "                )\n",
    "                print(f\"{model_name} loaded successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load model: {e}\")\n",
    "                print(\"Falling back to mock LLM\")\n",
    "                self.use_real_model = False\n",
    "    \n",
    "    def get_object_prior(self, object_name: str, locations: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Get probability distribution for where object might be\"\"\"\n",
    "        \n",
    "        # Hand-coded rules\n",
    "        priors = {\n",
    "            \"apple\": {\"kitchen\": 0.5, \"fridge\": 0.3, \"pantry\": 0.2},\n",
    "            \"book\": {\"bedroom\": 0.4, \"living_room\": 0.4, \"office\": 0.2},\n",
    "            \"keys\": {\"entrance\": 0.4, \"bedroom\": 0.3, \"kitchen\": 0.3},\n",
    "            \"phone\": {\"bedroom\": 0.3, \"living_room\": 0.3, \"office\": 0.4},\n",
    "            \"wallet\": {\"bedroom\": 0.5, \"entrance\": 0.3, \"office\": 0.2},\n",
    "            \"plate\": {\"kitchen\": 0.6, \"dishwasher\": 0.3, \"cabinet\": 0.1}\n",
    "        }\n",
    "        \n",
    "        if object_name in priors:\n",
    "            result = {}\n",
    "            for loc in locations:\n",
    "                for key in priors[object_name]:\n",
    "                    if key in loc.lower() or loc.lower() in key:\n",
    "                        result[loc] = priors[object_name].get(key, 0)\n",
    "            \n",
    "            if result:\n",
    "                total = sum(result.values())\n",
    "                return {k: v/total for k, v in result.items()}\n",
    "        \n",
    "        # Uniform distribution for unknown objects\n",
    "        uniform_prob = 1.0 / len(locations)\n",
    "        return {loc: uniform_prob for loc in locations}\n",
    "    \n",
    "    def suggest_action(self, state: Dict, goal: str, history: List[str]) -> str:\n",
    "        \"\"\"Suggest next action based on state and goal\"\"\"\n",
    "        \n",
    "        # If we can see objects and not holding anything, pick them up\n",
    "        if state.get('visible_objects') and not state.get('holding'):\n",
    "            return f\"pick_{state['visible_objects'][0]}\"\n",
    "    \n",
    "        # If holding something, place it\n",
    "        if state.get('holding'):\n",
    "            return f\"place_{state['holding']}\"\n",
    "        \n",
    "        # Move to a new location\n",
    "        rooms = [\"kitchen\", \"bedroom\", \"living_room\", \"bathroom\", \"office\"]\n",
    "        current = state.get('robot_location', '')\n",
    "        available_rooms = [r for r in rooms if r != current]\n",
    "        \n",
    "        if available_rooms:\n",
    "            return f\"move_to_{random.choice(available_rooms)}\"\n",
    "        \n",
    "        return \"search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de6c75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    state: Dict\n",
    "    action: Optional[str]\n",
    "    parent: Optional['Node']\n",
    "    children: List['Node'] = field(default_factory=list)\n",
    "    visits: int = 0\n",
    "    value: float = 0.0\n",
    "\n",
    "class SimpleMCTS:\n",
    "    def __init__(self, llm, c_param=1.4, max_simulations=50):\n",
    "        self.llm = llm\n",
    "        self.c_param = c_param\n",
    "        self.max_simulations = max_simulations\n",
    "    \n",
    "    def search(self, initial_state: Dict, goal: Dict, available_actions: List[str]) -> str:\n",
    "        #Run MCTS and return best action\n",
    "        root = Node(state=initial_state, action=None, parent=None)\n",
    "        \n",
    "        for sim in range(self.max_simulations):\n",
    "            node = self._select(root)\n",
    "            \n",
    "            if node.visits > 0 and len(available_actions) > 0:\n",
    "                node = self._expand(node, available_actions)\n",
    "            \n",
    "            reward = self._simulate(node.state, goal)\n",
    "            self._backpropagate(node, reward)\n",
    "        \n",
    "        return self._best_action(root)\n",
    "    \n",
    "    def _select(self, node: Node) -> Node:\n",
    "        #Select node using UCT\n",
    "        while node.children:\n",
    "            if all(child.visits > 0 for child in node.children):\n",
    "                node = self._uct_select(node)\n",
    "            else:\n",
    "                return next(c for c in node.children if c.visits == 0)\n",
    "        return node\n",
    "    \n",
    "    def _uct_select(self, node: Node) -> Node:\n",
    "        #Select child with highest UCT value\n",
    "        best_value = -float('inf')\n",
    "        best_node = None\n",
    "        \n",
    "        for child in node.children:\n",
    "            if child.visits == 0:\n",
    "                return child\n",
    "            uct_value = (child.value / child.visits + \n",
    "                        self.c_param * math.sqrt(math.log(node.visits) / child.visits))\n",
    "            if uct_value > best_value:\n",
    "                best_value = uct_value\n",
    "                best_node = child\n",
    "        \n",
    "        return best_node\n",
    "    \n",
    "    def _expand(self, node: Node, actions: List[str]) -> Node:\n",
    "        #Expand node with one child\n",
    "        suggested_action = self.llm.suggest_action(node.state, \"goal\", [])\n",
    "        \n",
    "        if suggested_action not in actions:\n",
    "            suggested_action = random.choice(actions) if actions else \"search\"\n",
    "        \n",
    "        new_state = self._apply_action(node.state.copy(), suggested_action)\n",
    "        child = Node(state=new_state, action=suggested_action, parent=node)\n",
    "        node.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def _simulate(self, state: Dict, goal: Dict) -> float:\n",
    "        \"\"\"Random rollout from state\"\"\"\n",
    "        current_state = state.copy()\n",
    "        steps = 0\n",
    "        max_steps = 20\n",
    "    \n",
    "        # Immediate rewards based on current state\n",
    "        if current_state.get('holding'):\n",
    "            # Holding an object is good progress\n",
    "            return 0.4 - (steps / max_steps) * 0.1\n",
    "    \n",
    "        # Simulate random actions\n",
    "        while steps < max_steps:\n",
    "            if current_state.get('visible_objects'):\n",
    "                # Seeing objects is partial success\n",
    "                return 0.2 - (steps / max_steps) * 0.1\n",
    "        \n",
    "            # Simple state evolution\n",
    "            if random.random() < 0.3:  # 30% chance of progress\n",
    "                return 0.1 - (steps / max_steps) * 0.05\n",
    "        \n",
    "            steps += 1\n",
    "    \n",
    "        return 0.0\n",
    "    \n",
    "    def _backpropagate(self, node: Node, reward: float):\n",
    "        #Update values up the tree\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value += reward\n",
    "            node = node.parent\n",
    "    \n",
    "    def _best_action(self, root: Node) -> str:\n",
    "        \"\"\"Select most visited child\"\"\"\n",
    "        if not root.children:\n",
    "            return \"search\"  # Default action\n",
    "        \n",
    "        best_child = max(root.children, key=lambda c: c.visits)\n",
    "        return best_child.action if best_child.action else \"search\"\n",
    "    \n",
    "    def _apply_action(self, state: Dict, action: str) -> Dict:\n",
    "        \"\"\"Apply action to state (simplified simulation)\"\"\"\n",
    "        new_state = state.copy()\n",
    "        new_state[\"last_action\"] = action\n",
    "        \n",
    "        # Simple state transitions for simulation\n",
    "        if \"move\" in action:\n",
    "            rooms = [\"kitchen\", \"bedroom\", \"living_room\", \"bathroom\", \"office\"]\n",
    "            new_state[\"robot_location\"] = random.choice(rooms)\n",
    "        elif \"pick\" in action and state.get(\"visible_objects\"):\n",
    "            new_state[\"holding\"] = state[\"visible_objects\"][0] if state.get(\"visible_objects\") else None\n",
    "        elif \"place\" in action:\n",
    "            new_state[\"holding\"] = None\n",
    "            \n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afaff8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectRearrangementTask:\n",
    "    \"\"\"Task: Rearrange objects to target locations\"\"\"\n",
    "    \n",
    "    def __init__(self, n_objects=3, n_rooms=5):\n",
    "        self.rooms = [\"kitchen\", \"bedroom\", \"living_room\", \"bathroom\", \"office\"][:n_rooms]\n",
    "        self.containers = {\n",
    "            \"kitchen\": [\"fridge\", \"cabinet\", \"counter\"],\n",
    "            \"bedroom\": [\"dresser\", \"nightstand\"],\n",
    "            \"living_room\": [\"coffee_table\", \"shelf\"],\n",
    "            \"bathroom\": [\"medicine_cabinet\"],\n",
    "            \"office\": [\"desk\", \"bookshelf\"]\n",
    "        }\n",
    "        \n",
    "        self.objects = [\"apple\", \"book\", \"keys\", \"mug\", \"phone\"][:n_objects]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"Reset environment to random state\"\"\"\n",
    "        if seed:\n",
    "            random.seed(seed)\n",
    "            \n",
    "        self.robot_location = \"kitchen\"\n",
    "        self.holding = None\n",
    "        self.steps_taken = 0\n",
    "        self.completed_goals = set()  # ADD THIS LINE - tracks completed objects\n",
    "        \n",
    "        # Random initial placement\n",
    "        self.object_locations = {}\n",
    "        for obj in self.objects:\n",
    "            room = random.choice(self.rooms)\n",
    "            container = random.choice(self.containers.get(room, [\"floor\"]))\n",
    "            self.object_locations[obj] = (room, container)\n",
    "        \n",
    "        # Set goals\n",
    "        self.goal_locations = {\n",
    "            \"apple\": (\"kitchen\", \"fridge\"),\n",
    "            \"book\": (\"office\", \"bookshelf\"),\n",
    "            \"keys\": (\"bedroom\", \"nightstand\"),\n",
    "        }\n",
    "        \n",
    "        self.observed_locations = {}\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self) -> Dict:\n",
    "        \"\"\"Get current partially observable state\"\"\"\n",
    "        visible_objects = self._get_visible_objects()\n",
    "        \n",
    "        state = {\n",
    "            \"robot_location\": self.robot_location,\n",
    "            \"holding\": self.holding,\n",
    "            \"visible_objects\": visible_objects,\n",
    "            \"observed\": self.observed_locations.copy(),\n",
    "            \"steps\": self.steps_taken\n",
    "        }\n",
    "        \n",
    "        for obj in visible_objects:\n",
    "            self.observed_locations[obj] = self.object_locations[obj]\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _get_visible_objects(self) -> List[str]:\n",
    "        \"\"\"Objects visible in current room\"\"\"\n",
    "        visible = []\n",
    "        for obj, (room, container) in self.object_locations.items():\n",
    "            if room == self.robot_location and obj != self.holding:\n",
    "                # CHANGE: Don't show completed objects\n",
    "                if obj not in self.completed_goals:\n",
    "                    visible.append(obj)\n",
    "        return visible\n",
    "    \n",
    "    def execute_action(self, action: str) -> Tuple[Dict, float, bool, Dict]:\n",
    "        \"\"\"Execute action and return (state, reward, done, info)\"\"\"\n",
    "        self.steps_taken += 1\n",
    "        reward = -0.01  # Step penalty\n",
    "        info = {\"success\": False, \"action\": action}\n",
    "        \n",
    "        if action.startswith(\"move_to_\"):\n",
    "            room = action.replace(\"move_to_\", \"\")\n",
    "            if room in self.rooms:\n",
    "                self.robot_location = room\n",
    "        \n",
    "        elif action.startswith(\"pick_\"):\n",
    "            obj = action.replace(\"pick_\", \"\")\n",
    "            if obj in self._get_visible_objects() and not self.holding:\n",
    "                self.holding = obj\n",
    "                #del self.object_locations[obj]\n",
    "                reward = 0.1\n",
    "        \n",
    "        elif action.startswith(\"place_\"):\n",
    "            if self.holding:\n",
    "                obj_to_place = self.holding  # CHANGE: Store the object\n",
    "                self.object_locations[obj_to_place] = (self.robot_location, \"floor\")\n",
    "                \n",
    "                # CHANGE: Check if goal achieved and not already completed\n",
    "                if obj_to_place in self.goal_locations and obj_to_place not in self.completed_goals:\n",
    "                    goal_room, goal_container = self.goal_locations[obj_to_place]\n",
    "                    if self.robot_location == goal_room:\n",
    "                        reward = 1.0\n",
    "                        info[\"success\"] = True\n",
    "                        self.completed_goals.add(obj_to_place)  # Mark as completed\n",
    "                \n",
    "                self.holding = None\n",
    "        \n",
    "        done = self._check_all_goals() or self.steps_taken >= 100\n",
    "        return self.get_state(), reward, done, info\n",
    "    \n",
    "    def _check_all_goals(self) -> bool:\n",
    "        \"\"\"Check if all goals achieved\"\"\"\n",
    "        # CHANGE: Use completed_goals to check\n",
    "        for obj in self.objects:\n",
    "            if obj in self.goal_locations and obj not in self.completed_goals:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def get_available_actions(self) -> List[str]:\n",
    "        \"\"\"Get valid actions\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        for room in self.rooms:\n",
    "            if room != self.robot_location:\n",
    "                actions.append(f\"move_to_{room}\")\n",
    "        \n",
    "        if not self.holding:\n",
    "            for obj in self._get_visible_objects():\n",
    "                actions.append(f\"pick_{obj}\")\n",
    "        elif self.holding:\n",
    "            actions.append(f\"place_{self.holding}\")\n",
    "        \n",
    "        actions.append(\"search\")\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67a8e751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LLM-MCTS EXPERIMENT - Object Rearrangement Task\n",
      "============================================================\n",
      "\n",
      "Testing RANDOM Agent...\n",
      "  Trial 1: Reward=0.78, Steps=100\n",
      "  Trial 2: Reward=0.23, Steps=100\n",
      "  Trial 3: Reward=0.34, Steps=100\n",
      "\n",
      "Testing LLM_POLICY Agent...\n",
      "  Trial 1: Reward=4.50, Steps=100\n",
      "  Trial 2: Reward=4.50, Steps=100\n",
      "  Trial 3: Reward=4.50, Steps=100\n",
      "\n",
      "Testing LLM_MCTS Agent...\n",
      "  Trial 1: Reward=4.50, Steps=100\n",
      "  Trial 2: Reward=5.40, Steps=100\n",
      "  Trial 3: Reward=5.51, Steps=100\n",
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "RANDOM:\n",
      "  Avg Reward:    0.450\n",
      "  Avg Steps:     100.0\n",
      "  Success Rate:  0%\n",
      "\n",
      "LLM_POLICY:\n",
      "  Avg Reward:    4.500\n",
      "  Avg Steps:     100.0\n",
      "  Success Rate:  0%\n",
      "\n",
      "LLM_MCTS:\n",
      "  Avg Reward:    5.137\n",
      "  Avg Steps:     100.0\n",
      "  Success Rate:  0%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(agent_type, task, llm=None, mcts=None, max_steps=100):\n",
    "    #Run a single experiment\n",
    "    task.reset(seed=random.randint(0, 1000))\n",
    "    \n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        state = task.get_state()\n",
    "        actions = task.get_available_actions()\n",
    "        \n",
    "        if agent_type == \"random\":\n",
    "            action = random.choice(actions)\n",
    "        elif agent_type == \"llm_policy\":\n",
    "            action = llm.suggest_action(state, \"goal\", [])\n",
    "            if action not in actions:\n",
    "                action = random.choice(actions)\n",
    "        elif agent_type == \"llm_mcts\":\n",
    "            action = mcts.search(state, {\"complete\": True}, actions)\n",
    "        \n",
    "        state, reward, done, info = task.execute_action(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"total_reward\": total_reward,\n",
    "        \"steps\": steps,\n",
    "        \"completed\": task._check_all_goals()\n",
    "    }\n",
    "\n",
    "# Run main experiment\n",
    "print(\"=\" * 60)\n",
    "print(\"LLM-MCTS EXPERIMENT - Object Rearrangement Task\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize\n",
    "use_real_llm = False  # Set to True to use GPT-2\n",
    "n_trials = 3\n",
    "n_objects = 2\n",
    "\n",
    "llm = SimpleLLM(use_real_model=use_real_llm)\n",
    "mcts = SimpleMCTS(llm, max_simulations=20)\n",
    "task = ObjectRearrangementTask(n_objects=n_objects)\n",
    "\n",
    "# Run experiments\n",
    "agents = [\"random\", \"llm_policy\", \"llm_mcts\"]\n",
    "results = {agent: [] for agent in agents}\n",
    "\n",
    "for agent_type in agents:\n",
    "    print(f\"\\nTesting {agent_type.upper()} Agent...\")\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        result = run_experiment(agent_type, task, llm, mcts)\n",
    "        results[agent_type].append(result)\n",
    "        print(f\"  Trial {trial + 1}: Reward={result['total_reward']:.2f}, Steps={result['steps']}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for agent_type in agents:\n",
    "    agent_results = results[agent_type]\n",
    "    avg_reward = sum(r['total_reward'] for r in agent_results) / n_trials\n",
    "    avg_steps = sum(r['steps'] for r in agent_results) / n_trials\n",
    "    success_rate = sum(r['completed'] for r in agent_results) / n_trials * 100\n",
    "    \n",
    "    print(f\"\\n{agent_type.upper()}:\")\n",
    "    print(f\"  Avg Reward:    {avg_reward:.3f}\")\n",
    "    print(f\"  Avg Steps:     {avg_steps:.1f}\")\n",
    "    print(f\"  Success Rate:  {success_rate:.0f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
